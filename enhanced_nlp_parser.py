#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆè‡ªç„¶è¯­è¨€è§£æå™¨
çªç ´å½“å‰ç³»ç»Ÿçš„é™åˆ¶ï¼Œæ”¯æŒæ›´å¤æ‚çš„è‡ªç„¶è¯­è¨€æè¿°
"""

import re
from typing import Dict, List, Tuple, Any
import pandas as pd
from dataclasses import dataclass

@dataclass
class ParsedCondition:
    """è§£æåçš„æ¡ä»¶"""
    field: str
    operator: str
    value: Any
    confidence: float = 1.0
    description: str = ""

class EnhancedNLPParser:
    """å¢å¼ºç‰ˆè‡ªç„¶è¯­è¨€è§£æå™¨"""
    
    def __init__(self):
        # åˆå§‹åŒ–jiebaåˆ†è¯
        self._init_jieba()
        
        # å­—æ®µæ˜ å°„è¯å…¸
        self.field_synonyms = {
            'price': ['è‚¡ä»·', 'ä»·æ ¼', 'æ”¶ç›˜ä»·', 'ç°ä»·', 'æœ€æ–°ä»·', 'è‚¡ç¥¨ä»·æ ¼'],
            'change_pct': ['æ¶¨å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨å¹…åº¦', 'æ¶¨è·Œ', 'æ¶¨è·Œç‡', 'å˜åŠ¨å¹…åº¦'],
            'volume': ['æˆäº¤é‡', 'äº¤æ˜“é‡', 'æˆäº¤é¢', 'äº¤æ˜“é¢', 'æˆäº¤æ‰‹æ•°'],
            'market_cap': ['å¸‚å€¼', 'æ€»å¸‚å€¼', 'æµé€šå¸‚å€¼', 'å¸‚åœºä»·å€¼'],
            'pe_ratio': ['PE', 'å¸‚ç›ˆç‡', 'åŠ¨æ€å¸‚ç›ˆç‡', 'é™æ€å¸‚ç›ˆç‡'],
            'pb_ratio': ['PB', 'å¸‚å‡€ç‡', 'è´¦é¢ä»·å€¼æ¯”'],
            'turnover_rate': ['æ¢æ‰‹ç‡', 'æµé€šç‡'],
            'rsi': ['RSI', 'ç›¸å¯¹å¼ºå¼±æŒ‡æ•°', 'å¼ºå¼±æŒ‡æ ‡'],
            'ma': ['å‡çº¿', 'ç§»åŠ¨å¹³å‡çº¿', 'MA', 'å¹³å‡çº¿']
        }
        
        # æ“ä½œç¬¦æ˜ å°„
        self.operator_synonyms = {
            '>': ['å¤§äº', 'é«˜äº', 'è¶…è¿‡', 'å¤šäº', 'è¶…è¶Š', 'é«˜å‡º', '>', 'ï¼'],
            '<': ['å°äº', 'ä½äº', 'å°‘äº', 'ä¸è¶³', 'ä½è¿‡', '<', 'ï¼œ'],
            '>=': ['å¤§äºç­‰äº', 'ä¸å°‘äº', 'è‡³å°‘', 'ä¸ä½äº', '>=', 'â‰¥'],
            '<=': ['å°äºç­‰äº', 'ä¸è¶…è¿‡', 'è‡³å¤š', 'ä¸é«˜äº', '<=', 'â‰¤'],
            '=': ['ç­‰äº', 'æ˜¯', 'ä¸º', '=', 'ï¼'],
            'between': ['ä¹‹é—´', 'ä»‹äº', 'åœ¨...ä¹‹é—´', 'ä»...åˆ°', 'èŒƒå›´']
        }
        
        # æ•°å€¼å•ä½è½¬æ¢
        self.unit_multipliers = {
            'ä¸‡': 10000,
            'åä¸‡': 100000,
            'ç™¾ä¸‡': 1000000,
            'åƒä¸‡': 10000000,
            'äº¿': 100000000,
            'åäº¿': 1000000000,
            'ç™¾äº¿': 10000000000,
            'åƒäº¿': 100000000000,
            'ä¸‡äº¿': 1000000000000
        }
        
        # æ¨¡ç³Šæ¦‚å¿µæ˜ å°„
        self.fuzzy_concepts = {
            'å¤§ç›˜è‚¡': {'field': 'market_cap', 'operator': '>', 'value': 50000000000},  # 500äº¿ä»¥ä¸Š
            'ä¸­ç›˜è‚¡': {'field': 'market_cap', 'operator': 'between', 'value': [10000000000, 50000000000]},
            'å°ç›˜è‚¡': {'field': 'market_cap', 'operator': '<', 'value': 10000000000},  # 100äº¿ä»¥ä¸‹
            'é«˜ä»·è‚¡': {'field': 'current_price', 'operator': '>', 'value': 50},
            'ä¸­ä»·è‚¡': {'field': 'current_price', 'operator': 'between', 'value': [10, 50]},
            'ä½ä»·è‚¡': {'field': 'current_price', 'operator': '<', 'value': 10},
            'æ´»è·ƒè‚¡': {'field': 'turnover_rate', 'operator': '>', 'value': 5},
            'ä¸é”™çš„': {'field': 'change_pct', 'operator': '>', 'value': 2},
            'è¡¨ç°å¥½': {'field': 'change_pct', 'operator': '>', 'value': 3},
            'æœ‰æ½œåŠ›': {'field': 'pe_ratio', 'operator': '<', 'value': 30},
            'ä»·å€¼è‚¡': {'field': 'pb_ratio', 'operator': '<', 'value': 2},
            'æˆé•¿è‚¡': {'field': 'pe_ratio', 'operator': 'between', 'value': [15, 40]}
        }
        
        # è¡Œä¸šæ¦‚å¿µè¯å…¸ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.industry_concepts = {
            'æ–°èƒ½æº': ['æ–°èƒ½æº', 'ç”µåŠ¨è½¦', 'é”‚ç”µæ± ', 'å…‰ä¼', 'é£ç”µ', 'å‚¨èƒ½'],
            'ç§‘æŠ€è‚¡': ['ç§‘æŠ€', 'äº’è”ç½‘', 'äººå·¥æ™ºèƒ½', 'AI', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“'],
            'åŒ»è¯è‚¡': ['åŒ»è¯', 'ç”Ÿç‰©åŒ»è¯', 'ç–«è‹—', 'åŒ»ç–—å™¨æ¢°', 'ä¸­è¯'],
            'é‡‘èè‚¡': ['é“¶è¡Œ', 'ä¿é™©', 'è¯åˆ¸', 'ä¿¡æ‰˜', 'åŸºé‡‘'],
            'åœ°äº§è‚¡': ['æˆ¿åœ°äº§', 'å»ºç­‘', 'è£…ä¿®', 'ç‰©ä¸š'],
            'æ¶ˆè´¹è‚¡': ['ç™½é…’', 'é£Ÿå“', 'é›¶å”®', 'æœè£…', 'å®¶ç”µ']
        }
        
        # å¦å®šè¯
        self.negation_words = ['ä¸', 'ä¸è¦', 'é¿å…', 'æ’é™¤', 'é', 'æ— ', 'æ²¡æœ‰']
        
        # é€»è¾‘è¿æ¥è¯
        self.logic_connectors = {
            'AND': ['ä¸”', 'å’Œ', 'å¹¶ä¸”', 'åŒæ—¶', 'ä»¥åŠ', '&', '&&'],
            'OR': ['æˆ–', 'æˆ–è€…', 'è¦ä¹ˆ', '|', '||']
        }
    
    def _init_jieba(self):
        """åˆå§‹åŒ–åˆ†è¯å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è‚¡ç¥¨ç›¸å…³è¯æ±‡åˆ—è¡¨ï¼ˆç”¨äºåç»­å¤„ç†ï¼‰
        self.stock_words = [
            'è‚¡ä»·', 'æ¶¨å¹…', 'å¸‚å€¼', 'å¸‚ç›ˆç‡', 'å¸‚å‡€ç‡', 'æˆäº¤é‡', 'æ¢æ‰‹ç‡',
            'RSI', 'MACD', 'å‡çº¿', 'å¤§ç›˜è‚¡', 'å°ç›˜è‚¡', 'æ–°èƒ½æº', 'ç§‘æŠ€è‚¡'
        ]
    
    def parse_rule(self, rule_text: str) -> Dict:
        """è§£æè‡ªç„¶è¯­è¨€è§„åˆ™"""
        print(f"\nğŸ” å¼€å§‹è§£æè§„åˆ™: {rule_text}")
        
        # é¢„å¤„ç†
        processed_text = self._preprocess_text(rule_text)
        print(f"ğŸ“ é¢„å¤„ç†å: {processed_text}")
        
        # åˆ†è¯å’Œè¯æ€§æ ‡æ³¨
        words = self._segment_text(processed_text)
        print(f"ğŸ”¤ åˆ†è¯ç»“æœ: {words}")
        
        # è¯†åˆ«é€»è¾‘å…³ç³»
        logic_type = self._detect_logic(processed_text)
        print(f"ğŸ”— é€»è¾‘å…³ç³»: {logic_type}")
        
        # æå–æ¡ä»¶
        conditions = self._extract_conditions(processed_text, words)
        print(f"ğŸ“Š æå–æ¡ä»¶: {len(conditions)}ä¸ª")
        
        # å¤„ç†æ¨¡ç³Šæ¦‚å¿µ
        fuzzy_conditions = self._handle_fuzzy_concepts(processed_text)
        conditions.extend(fuzzy_conditions)
        
        # å¤„ç†è¡Œä¸šæ¦‚å¿µ
        industry_conditions = self._handle_industry_concepts(processed_text)
        conditions.extend(industry_conditions)
        
        return {
            'original_text': rule_text,
            'processed_text': processed_text,
            'conditions': [self._condition_to_dict(c) for c in conditions],
            'logic': logic_type,
            'confidence': self._calculate_confidence(conditions)
        }
    
    def _preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        # ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
        text = text.replace('ï¼Œ', ',')
        text = text.replace('ã€‚', '.')
        text = text.replace('ï¼›', ';')
        text = text.replace('ï¼š', ':')
        
        # å¤„ç†æ•°å­—è¡¨è¾¾
        text = self._normalize_numbers(text)
        
        # å¤„ç†å¦å®šè¡¨è¾¾
        text = self._handle_negations(text)
        
        return text.strip()
    
    def _normalize_numbers(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ•°å­—è¡¨è¾¾"""
        # ä¸­æ–‡æ•°å­—è½¬é˜¿æ‹‰ä¼¯æ•°å­—
        chinese_numbers = {
            'ä¸€': '1', 'äºŒ': '2', 'ä¸‰': '3', 'å››': '4', 'äº”': '5',
            'å…­': '6', 'ä¸ƒ': '7', 'å…«': '8', 'ä¹': '9', 'å': '10',
            'ç™¾': '100', 'åƒ': '1000'
        }
        
        for cn, num in chinese_numbers.items():
            text = text.replace(cn, num)
        
        # å¤„ç†å¤åˆæ•°å­—è¡¨è¾¾
        patterns = [
            (r'(\d+)å(\d+)', lambda m: str(int(m.group(1)) * 10 + int(m.group(2)))),
            (r'(\d+)ç™¾(\d+)å(\d+)', lambda m: str(int(m.group(1)) * 100 + int(m.group(2)) * 10 + int(m.group(3)))),
        ]
        
        for pattern, replacement in patterns:
            text = re.sub(pattern, replacement, text)
        
        return text
    
    def _handle_negations(self, text: str) -> str:
        """å¤„ç†å¦å®šè¡¨è¾¾"""
        # ç®€å•çš„å¦å®šå¤„ç†ï¼Œå°†"ä¸è¦å¤ªé«˜"è½¬æ¢ä¸º"å°äºæŸä¸ªå€¼"
        negation_patterns = [
            (r'ä¸è¦å¤ªé«˜', 'å°äº50'),
            (r'ä¸è¦å¤ªä½', 'å¤§äº5'),
            (r'é¿å…é«˜ä»·', 'å°äº30'),
            (r'æ’é™¤ä½ä»·', 'å¤§äº10')
        ]
        
        for pattern, replacement in negation_patterns:
            text = re.sub(pattern, replacement, text)
        
        return text
    
    def _segment_text(self, text: str) -> List[Tuple[str, str]]:
        """ç®€å•åˆ†è¯å’Œè¯æ€§æ ‡æ³¨"""
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œç®€å•åˆ†è¯
        tokens = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+|\d+(?:\.\d+)?|[><=â‰¥â‰¤]', text)
        # ç®€å•çš„è¯æ€§æ ‡æ³¨ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        result = []
        for token in tokens:
            if re.match(r'\d+(?:\.\d+)?', token):
                result.append((token, 'm'))  # æ•°è¯
            elif token in ['>', '<', '>=', '<=', '=', 'â‰¥', 'â‰¤']:
                result.append((token, 'p'))  # ä»‹è¯
            else:
                result.append((token, 'n'))  # åè¯
        return result
    
    def _detect_logic(self, text: str) -> str:
        """æ£€æµ‹é€»è¾‘å…³ç³»"""
        # æ£€æŸ¥ORé€»è¾‘
        for connector in self.logic_connectors['OR']:
            if connector in text:
                return 'OR'
        
        # é»˜è®¤ANDé€»è¾‘
        return 'AND'
    
    def _extract_conditions(self, text: str, words: List[Tuple[str, str]]) -> List[ParsedCondition]:
        """æå–æ¡ä»¶"""
        conditions = []
        
        # ä½¿ç”¨æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        patterns = self._get_enhanced_patterns()
        
        for pattern_name, pattern in patterns.items():
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                condition = self._create_condition_from_match(pattern_name, match)
                if condition:
                    conditions.append(condition)
        
        return conditions
    
    def _get_enhanced_patterns(self) -> Dict[str, str]:
        """è·å–å¢å¼ºçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼"""
        return {
            # ä»·æ ¼ç›¸å…³ - æ”¯æŒæ›´å¤šè¡¨è¾¾æ–¹å¼
            'price_above': r'(?:è‚¡ä»·|ä»·æ ¼|æ”¶ç›˜ä»·|ç°ä»·|æœ€æ–°ä»·)(?:è¦|éœ€è¦|åº”è¯¥)?(?:å¤§äº|é«˜äº|è¶…è¿‡|å¤šäº|>|ï¼)\s*(\d+(?:\.\d+)?)(?:å…ƒ|å—)?',
            'price_below': r'(?:è‚¡ä»·|ä»·æ ¼|æ”¶ç›˜ä»·|ç°ä»·|æœ€æ–°ä»·)(?:è¦|éœ€è¦|åº”è¯¥)?(?:å°äº|ä½äº|å°‘äº|ä¸è¶³|<|ï¼œ)\s*(\d+(?:\.\d+)?)(?:å…ƒ|å—)?',
            'price_range': r'(?:è‚¡ä»·|ä»·æ ¼|æ”¶ç›˜ä»·)(?:åœ¨|ä»‹äº|ä»)\s*(\d+(?:\.\d+)?)(?:å…ƒ|å—)?(?:åˆ°|è‡³|-|~)\s*(\d+(?:\.\d+)?)(?:å…ƒ|å—)?(?:ä¹‹é—´|èŒƒå›´)?',
            
            # æ¶¨è·Œå¹…ç›¸å…³ - æ”¯æŒæ›´å¤šè¡¨è¾¾
            'change_above': r'(?:æ¶¨å¹…|æ¶¨è·Œå¹…|æ¶¨å¹…åº¦|æ¶¨è·Œ|ä¸Šæ¶¨)(?:è¦|éœ€è¦|åº”è¯¥)?(?:å¤§äº|é«˜äº|è¶…è¿‡|å¤šäº|>|ï¼)\s*(\d+(?:\.\d+)?)(?:%|ä¸ªç™¾åˆ†ç‚¹|ç™¾åˆ†ç‚¹)?',
            'change_positive': r'(?:ä¸Šæ¶¨|æ¶¨|æ­£æ¶¨å¹…|æ¶¨å¹…ä¸ºæ­£)',
            
            # å¸‚å€¼ç›¸å…³ - æ”¯æŒä¸­æ–‡æ•°å­—
            'market_cap_above': r'(?:å¸‚å€¼|æ€»å¸‚å€¼)(?:è¦|éœ€è¦|åº”è¯¥)?(?:å¤§äº|é«˜äº|è¶…è¿‡|å¤šäº|>|ï¼)\s*(\d+(?:\.\d+)?)(?:ä¸‡|åä¸‡|ç™¾ä¸‡|åƒä¸‡|äº¿|åäº¿|ç™¾äº¿|åƒäº¿|ä¸‡äº¿)?(?:å…ƒ)?',
            'market_cap_below': r'(?:å¸‚å€¼|æ€»å¸‚å€¼)(?:è¦|éœ€è¦|åº”è¯¥)?(?:å°äº|ä½äº|å°‘äº|ä¸è¶³|<|ï¼œ)\s*(\d+(?:\.\d+)?)(?:ä¸‡|åä¸‡|ç™¾ä¸‡|åƒä¸‡|äº¿|åäº¿|ç™¾äº¿|åƒäº¿|ä¸‡äº¿)?(?:å…ƒ)?',
            
            # PE/PBç›¸å…³
            'pe_below': r'(?:PE|å¸‚ç›ˆç‡|åŠ¨æ€å¸‚ç›ˆç‡)(?:è¦|éœ€è¦|åº”è¯¥)?(?:å°äº|ä½äº|å°‘äº|ä¸è¶³|<|ï¼œ)\s*(\d+(?:\.\d+)?)(?:å€)?',
            'pb_below': r'(?:PB|å¸‚å‡€ç‡)(?:è¦|éœ€è¦|åº”è¯¥)?(?:å°äº|ä½äº|å°‘äº|ä¸è¶³|<|ï¼œ)\s*(\d+(?:\.\d+)?)(?:å€)?',
            
            # æˆäº¤é‡ç›¸å…³
            'volume_above': r'(?:æˆäº¤é‡|äº¤æ˜“é‡)(?:è¦|éœ€è¦|åº”è¯¥)?(?:å¤§äº|é«˜äº|è¶…è¿‡|å¤šäº|>|ï¼)\s*(\d+(?:\.\d+)?)(?:ä¸‡|åƒä¸‡|äº¿)?(?:æ‰‹|è‚¡)?',
            
            # å¤åˆæ¡ä»¶
            'comprehensive_condition': r'(?:å¯»æ‰¾|æ‰¾|é€‰æ‹©|ç­›é€‰)(?:ä¸€äº›)?(.+?)(?:è‚¡ç¥¨|è‚¡|çš„è‚¡ç¥¨)',
        }
    
    def _create_condition_from_match(self, pattern_name: str, match) -> ParsedCondition:
        """ä»åŒ¹é…ç»“æœåˆ›å»ºæ¡ä»¶"""
        try:
            if pattern_name == 'price_above':
                return ParsedCondition(
                    field='current_price',
                    operator='>',
                    value=float(match.group(1)),
                    description=f"è‚¡ä»·å¤§äº{match.group(1)}å…ƒ"
                )
            elif pattern_name == 'price_below':
                return ParsedCondition(
                    field='current_price',
                    operator='<',
                    value=float(match.group(1)),
                    description=f"è‚¡ä»·å°äº{match.group(1)}å…ƒ"
                )
            elif pattern_name == 'price_range':
                return ParsedCondition(
                    field='current_price',
                    operator='between',
                    value=[float(match.group(1)), float(match.group(2))],
                    description=f"è‚¡ä»·åœ¨{match.group(1)}-{match.group(2)}å…ƒä¹‹é—´"
                )
            elif pattern_name == 'change_above':
                return ParsedCondition(
                    field='change_pct',
                    operator='>',
                    value=float(match.group(1)),
                    description=f"æ¶¨å¹…å¤§äº{match.group(1)}%"
                )
            elif pattern_name == 'change_positive':
                return ParsedCondition(
                    field='change_pct',
                    operator='>',
                    value=0,
                    description="æ¶¨å¹…ä¸ºæ­£"
                )
            elif pattern_name == 'market_cap_above':
                value = float(match.group(1))
                # å¤„ç†å•ä½
                unit_text = match.group(0)
                multiplier = self._get_unit_multiplier(unit_text)
                return ParsedCondition(
                    field='market_cap',
                    operator='>',
                    value=value * multiplier,
                    description=f"å¸‚å€¼å¤§äº{match.group(1)}{self._extract_unit(unit_text)}"
                )
            elif pattern_name == 'pe_below':
                return ParsedCondition(
                    field='pe_ratio',
                    operator='<',
                    value=float(match.group(1)),
                    description=f"PEå°äº{match.group(1)}"
                )
            elif pattern_name == 'pb_below':
                return ParsedCondition(
                    field='pb_ratio',
                    operator='<',
                    value=float(match.group(1)),
                    description=f"PBå°äº{match.group(1)}"
                )
            # å¯ä»¥ç»§ç»­æ·»åŠ æ›´å¤šæ¡ä»¶ç±»å‹
            
        except (ValueError, IndexError) as e:
            print(f"âš ï¸ åˆ›å»ºæ¡ä»¶å¤±è´¥: {e}")
            return None
        
        return None
    
    def _get_unit_multiplier(self, text: str) -> float:
        """è·å–å•ä½ä¹˜æ•°"""
        for unit, multiplier in self.unit_multipliers.items():
            if unit in text:
                return multiplier
        return 1.0
    
    def _extract_unit(self, text: str) -> str:
        """æå–å•ä½"""
        for unit in self.unit_multipliers.keys():
            if unit in text:
                return unit
        return ""
    
    def _handle_fuzzy_concepts(self, text: str) -> List[ParsedCondition]:
        """å¤„ç†æ¨¡ç³Šæ¦‚å¿µ"""
        conditions = []
        
        for concept, condition_def in self.fuzzy_concepts.items():
            if concept in text:
                condition = ParsedCondition(
                    field=condition_def['field'],
                    operator=condition_def['operator'],
                    value=condition_def['value'],
                    confidence=0.8,  # æ¨¡ç³Šæ¦‚å¿µç½®ä¿¡åº¦è¾ƒä½
                    description=f"æ¨¡ç³Šæ¦‚å¿µ: {concept}"
                )
                conditions.append(condition)
        
        return conditions
    
    def _handle_industry_concepts(self, text: str) -> List[ParsedCondition]:
        """å¤„ç†è¡Œä¸šæ¦‚å¿µ"""
        conditions = []
        
        for industry, keywords in self.industry_concepts.items():
            for keyword in keywords:
                if keyword in text:
                    # è¡Œä¸šæ¦‚å¿µè½¬æ¢ä¸ºç‰¹å®šæ¡ä»¶ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                    condition = ParsedCondition(
                        field='industry',
                        operator='=',
                        value=industry,
                        confidence=0.9,
                        description=f"è¡Œä¸šæ¦‚å¿µ: {industry}"
                    )
                    conditions.append(condition)
                    break
        
        return conditions
    
    def _condition_to_dict(self, condition: ParsedCondition) -> Dict:
        """å°†æ¡ä»¶å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'field': condition.field,
            'operator': condition.operator,
            'value': condition.value,
            'confidence': condition.confidence,
            'description': condition.description
        }
    
    def _calculate_confidence(self, conditions: List[ParsedCondition]) -> float:
        """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦"""
        if not conditions:
            return 0.0
        
        total_confidence = sum(c.confidence for c in conditions)
        return total_confidence / len(conditions)

# æµ‹è¯•å‡½æ•°
def test_enhanced_parser():
    """æµ‹è¯•å¢å¼ºç‰ˆè§£æå™¨"""
    parser = EnhancedNLPParser()
    
    test_cases = [
        "å¯»æ‰¾å¸‚å€¼è¶…è¿‡200äº¿å…ƒçš„å¤§ç›˜è‚¡ï¼Œè¦æ±‚å¸‚ç›ˆç‡ä½äº25å€ï¼Œå¸‚å‡€ç‡å°äº2å€",
        "ç­›é€‰è‚¡ä»·é«˜äº20å…ƒä½†ä½äº100å…ƒçš„ä¸­ä»·è‚¡ï¼Œæ¶¨å¹…è¦è¶…è¿‡3ä¸ªç™¾åˆ†ç‚¹",
        "æ‰¾ä¸€äº›ä¸é”™çš„è‚¡ç¥¨ï¼Œä»·æ ¼ä¸è¦å¤ªé«˜ï¼Œæ¶¨å¾—è¿˜å¯ä»¥",
        "å¸®æˆ‘é€‰æ‹©ä¸€äº›æœ‰æ½œåŠ›çš„ä¸­å°ç›˜è‚¡ç¥¨ï¼Œæœ€å¥½æ˜¯æœ€è¿‘è¡¨ç°ä¸é”™çš„",
        "æ–°èƒ½æºæ¦‚å¿µè‚¡ç¥¨ï¼Œè¦æ±‚æ¶¨å¹…å¤§äº5%ä¸”æˆäº¤é‡æ´»è·ƒ",
        "è‚¡ä»·åœ¨15åˆ°50å…ƒä¹‹é—´æˆ–è€…å¸‚å€¼å¤§äº100äº¿çš„è‚¡ç¥¨"
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•ç”¨ä¾‹ {i}: {test_case}")
        print('='*60)
        
        result = parser.parse_rule(test_case)
        
        print(f"\nâœ… è§£æç»“æœ:")
        print(f"ğŸ“ åŸå§‹æ–‡æœ¬: {result['original_text']}")
        print(f"ğŸ”„ å¤„ç†åæ–‡æœ¬: {result['processed_text']}")
        print(f"ğŸ”— é€»è¾‘å…³ç³»: {result['logic']}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦: {result['confidence']:.2f}")
        print(f"ğŸ¯ è¯†åˆ«æ¡ä»¶æ•°: {len(result['conditions'])}")
        
        for j, condition in enumerate(result['conditions'], 1):
            print(f"   {j}. {condition['description']} (ç½®ä¿¡åº¦: {condition['confidence']:.2f})")

if __name__ == "__main__":
    test_enhanced_parser()